{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "import string\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data importation and grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SMSSpamCollection.txt', 'r') as file:\n",
    "    csv_reader = csv.reader(file, delimiter = '\\t')\n",
    "    smsdata_data = [line[1] for line in csv_reader]\n",
    "    \n",
    "with open('SMSSpamCollection.txt', 'r') as file:\n",
    "    csv_reader = csv.reader(file, delimiter = '\\t')\n",
    "    smsdata_label = [line[0] for line in csv_reader]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The number of spam and ham messages present in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'ham': 4825, 'spam': 747})\n"
     ]
    }
   ],
   "source": [
    "label_count = Counter(smsdata_label)\n",
    "print(label_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocesssing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A brief step by step method of how to remove punctuations from a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T h i s . o n e ,   i s . . f o r   u ?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'This.one, is..for u?'\n",
    "b = ' '.join(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = [' ' if i in string.punctuation else i for i in a ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This one  is  for u '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_ = ''.join(z)\n",
    "z_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T h i s   o n e     i s     f o r   u  '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(z_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = [1,'.', 'a', '4']\n",
    "sent = 'This is a boy 4 u.'\n",
    "k = [' ' if i in ex else i for i in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is   boy   u '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ''.join(k)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is boy u'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(a.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### punctuation from our sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do note that we are doing this for each line in the data and not on the whole data at once. so to effect this, we will have to iterate through our data and apply the function in each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = smsdata_data[2]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry question std txt rate T C s apply 08452810075over18 s'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join((''.join([' ' if i in string.punctuation else i for i in t])).split())\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizing the sentenses based on white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [word for sent in sent_tokenize(text) for word in word_tokenize(sent)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### changing the words to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['free',\n",
       " 'entry',\n",
       " 'in',\n",
       " '2',\n",
       " 'a',\n",
       " 'wkly',\n",
       " 'comp',\n",
       " 'to',\n",
       " 'win',\n",
       " 'fa',\n",
       " 'cup',\n",
       " 'final',\n",
       " 'tkts',\n",
       " '21st',\n",
       " 'may',\n",
       " '2005',\n",
       " 'text',\n",
       " 'fa',\n",
       " 'to',\n",
       " '87121',\n",
       " 'to',\n",
       " 'receive',\n",
       " 'entry',\n",
       " 'question',\n",
       " 'std',\n",
       " 'txt',\n",
       " 'rate',\n",
       " 't',\n",
       " 'c',\n",
       " 's',\n",
       " 'apply',\n",
       " '08452810075over18',\n",
       " 's']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [word.lower() for word in tokens]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removal of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of tokens before removal of stopwords is 33 while the length after removal of stopwords is 25\n"
     ]
    }
   ],
   "source": [
    "new_tokens = [i for i in tokens if i not in stopwords.words('english')]\n",
    "print('The length of tokens before removal of stopwords is {} while the length after removal of stopwords is {}'.format(len(tokens), len(new_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep words with length greater than 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [word for word in new_tokens if len(word) >= 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokens = [stemmer.stem(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a prerequisite for lemmatization based on whether the word us noun or verb, this will reduce the word to the root word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_corpus = pos_tag(tokens)\n",
    "\n",
    "Noun_tags = ['NN', 'NNP', 'NNPS', 'NNS']\n",
    "verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prac_lemmatize(token, tag):\n",
    "    if tag in Noun_tags:\n",
    "        return lemmatizer.lemmatize(token, 'n')\n",
    "    elif tag in verb_tags:\n",
    "        return lemmatizer.lemmatize(token, 'v')\n",
    "    else:\n",
    "        return lemmatizer.lemmatize(token, 'n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'free entri wkli comp win cup final tkt 21st may 2005 text 87121 receiv entri question std txt rate appli 08452810075over18'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_text = ' '.join([prac_lemmatize(token, tag) for token, tag in tagged_corpus])\n",
    "lemmatized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinating the above text analysis steps into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    text = ' '.join((''.join([' ' if i in string.punctuation else i for i in text])).split())\n",
    "    tokens = [word for sent in sent_tokenize(text) for word in word_tokenize(sent)]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    new_tokens = [i for i in tokens if i not in stopwords.words('english')]\n",
    "    tokens = [word for word in new_tokens if len(word) >= 3]\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    tagged_corpus = pos_tag(tokens)\n",
    "\n",
    "    Noun_tags = ['NN', 'NNP', 'NNPS', 'NNS']\n",
    "    verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def prac_lemmatize(token, tag):\n",
    "        if tag in Noun_tags:\n",
    "            return lemmatizer.lemmatize(token, 'n')\n",
    "        elif tag in verb_tags:\n",
    "            return lemmatizer.lemmatize(token, 'v')\n",
    "        else:\n",
    "            return lemmatizer.lemmatize(token, 'n')\n",
    "    lemmatized_text = ' '.join([prac_lemmatize(token, tag) for token, tag in tagged_corpus])\n",
    "    \n",
    "    return lemmatized_text \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_sms_data = [text_preprocessing(i) for i in smsdata_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jurong point crazi avail bugi great world buffet cine get amor wat',\n",
       " 'lar joke wif oni',\n",
       " 'free entri wkli comp win cup final tkt 21st may 2005 text 87121 receiv entri question std txt rate appli 08452810075over18',\n",
       " 'dun say earli hor alreadi say',\n",
       " 'nah think goe usf live around though']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_sms_data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data ready to feed into the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size is 3900\n"
     ]
    }
   ],
   "source": [
    "train_set_size = int(round(len(preprocessed_sms_data) * 0.70))\n",
    "print('Train set size is', train_set_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitting the training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([i for i in preprocessed_sms_data[:train_set_size]])\n",
    "x_test = np.array([i for i in preprocessed_sms_data[train_set_size:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train =  np.array([i for i in smsdata_label[:train_set_size]])\n",
    "y_test =  np.array([i for i in smsdata_label[train_set_size:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converting the words into vectorizer format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words= 'english', max_features = 4000, strip_accents = 'unicode', norm = 'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2 = vectorizer.fit_transform(x_train).todense()\n",
    "X_test_2 = vectorizer.transform(x_test).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LGBMClassifier()\n",
    "model = clf.fit(X_train_2, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_train = clf.predict(X_train_2)\n",
    "predicted_test = clf.predict(X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9766746411483254"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(predicted_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I used lightgbm but feel free to use any classification model  of your choice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
